{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import sklearn.metrics as mx\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __score_model__(self, X_train, X_test, y_train, y_test, clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return mx.accuracy_score(y_test, y_pred)\n",
    "\n",
    "def ablation_test(self):\n",
    "    self.__remove_unusable_features__()\n",
    "    X = self.df.drop([self.label_type], axis=1)\n",
    "    y = self.df[self.label_type]\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    svc = SVC(kernel='rbf', gamma=1, C=0.1, random_state=0)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=6, p=2, weights='uniform')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    base_score = dict()\n",
    "    base_score['gnb'] = self.__score_model__(X_train, X_test, y_train, y_test, gnb)\n",
    "    base_score['svc'] = self.__score_model__(X_train, X_test, y_train, y_test, svc)\n",
    "    base_score['neigh'] = self.__score_model__(X_train, X_test, y_train, y_test, neigh)\n",
    "\n",
    "    scores = defaultdict(list)\n",
    "    for i in range(X_train.shape[1]):\n",
    "        cols = [ndx != i for ndx in range(X_train.shape[1])]\n",
    "        scores['gnb'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, gnb))\n",
    "        scores['svc'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, svc))\n",
    "        scores['neigh'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, neigh))\n",
    "\n",
    "    final_scores_gnb = dict()\n",
    "    final_scores_svc = dict()\n",
    "    final_scores_neigh = dict()\n",
    "    for k, v in scores.items():\n",
    "        if k == 'gnb':\n",
    "            for i in range(len(v)):\n",
    "                final_scores_gnb[X.columns[i]] = (v[i] - base_score[k])\n",
    "            final_scores_gnb = sorted(final_scores_gnb.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        elif k == 'svc':\n",
    "            for i in range(len(v)):\n",
    "                final_scores_svc[X.columns[i]] = (v[i] - base_score[k])\n",
    "            final_scores_svc = sorted(final_scores_svc.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        elif k == 'neigh':\n",
    "            for i in range(len(v)):\n",
    "                final_scores_neigh[X.columns[i]] = (v[i] - base_score[k])\n",
    "            final_scores_neigh = sorted(final_scores_neigh.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "    print('Based on NB: ', final_scores_gnb)\n",
    "    print('Based on SVC : ', final_scores_svc)\n",
    "    print('Based on KNN: ', final_scores_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __remove_unusable_features__(self):\n",
    "    print('Initial Shape is: ', self.df.shape)\n",
    "    if self.label_type == 'pvalue.label':\n",
    "        self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R', 'Meta.analysis.significant'], axis=1)\n",
    "    elif self.label_type == 'O.within.CI.R':\n",
    "        self.df = self.df.drop(['P.value.R', 'Direction.R', 'Meta.analysis.significant', 'pvalue.label'], axis=1)\n",
    "    elif self.label_type == 'Meta.analysis.significant':\n",
    "        self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R', 'pvalue.label'], axis=1)\n",
    "\n",
    "    cols_drop = set(['DOI', '1st.author.O', 'Senior.author.O', 'Study.Title.O', 'Unnamed: 0', 'Unnamed: 0.1', 'Volume.O'])\n",
    "    cols_total = set(self.df.columns)\n",
    "    self.df = self.df.drop(cols_drop.intersection(cols_total), axis=1)\n",
    "    self.df = self.df.dropna()\n",
    "    print('Shape after data cleaning is: ', self.df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __get_baseline__(self):\n",
    "    total_true = total_false = total_val = 0\n",
    "    for i, row in self.df.iterrows():\n",
    "        total_val += 1\n",
    "        if row[self.label_type] == 1:\n",
    "            total_true += 1\n",
    "        else:\n",
    "            total_false += 1\n",
    "    print(\"Total rows is= \", total_val)\n",
    "    print(\"Total papers reproducible\", total_true)\n",
    "    print(\"total_true % is= \", (total_true / total_val) * 100)\n",
    "    print(\"total_false % is=\", (total_false / total_val) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_features_chi2(self):\n",
    "    X = self.df.drop([self.label_type, 'Authors.O'], axis=1)\n",
    "    cols = X.columns\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    y = self.df[self.label_type]\n",
    "    bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "    fit = bestfeatures.fit(X, y)\n",
    "    dfscores = pandas.DataFrame(fit.scores_)\n",
    "    dfcolumns = pandas.DataFrame(cols)\n",
    "    featureScores = pandas.concat([dfcolumns, dfscores], axis=1)\n",
    "    featureScores.columns = ['Specs', 'Score']\n",
    "    final_features = featureScores.nlargest(featureScores.shape[0] - 2, 'Score')\n",
    "    pandas.set_option('display.max_rows', final_features.shape[0] + 1)\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_kfolds(self, train_set_size=0.9):\n",
    "    df = self.df.copy()\n",
    "    authors = set([j.strip() for i in list(df['Authors.O']) for j in i.split(',')])\n",
    "    train_set, test_set = pandas.DataFrame(columns=list(df.columns)), pandas.DataFrame(columns=list(df.columns))\n",
    "    while len(train_set) <= self.df.shape[0] * train_set_size and len(authors) > 0 and df.shape[0] > 0:\n",
    "        author = list(authors)[np.random.randint(0, len(authors))]\n",
    "        temp_df = df.loc[df['Authors.O'].str.contains(author)]\n",
    "        train_set = pandas.concat([train_set, temp_df], ignore_index=True)\n",
    "        authors.remove(author)\n",
    "\n",
    "        # remove all the rows from df that have been added to the new set\n",
    "        df.drop(df[df['Authors.O'].str.contains(author)].index, inplace=True)\n",
    "        \n",
    "        if test_set.shape[0] <= self.df.shape[0] * (1 - train_set_size) and len(authors) > 0 and df.shape[0] > 0:\n",
    "            author = list(authors)[np.random.randint(0, len(authors))]\n",
    "            temp_df = df.loc[df['Authors.O'].str.contains(author)]\n",
    "            test_set = pandas.concat([test_set, temp_df], ignore_index=True)\n",
    "            authors.remove(author)\n",
    "\n",
    "            # remove all the rows from df that have been added to the new set\n",
    "            df.drop(df[df['Authors.O'].str.contains(author)].index, inplace=True)\n",
    "            \n",
    "    train_set = train_set.drop(['Authors.O'], axis=1)\n",
    "    test_set = test_set.drop(['Authors.O'], axis=1)\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(self, best_features):\n",
    "    self.df = self.df.drop(['Authors.O'], axis=1)\n",
    "    X = self.df[best_features]\n",
    "    y = self.df[self.label_type]\n",
    "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    svc = SVC(kernel='rbf', gamma=0.9, C=1, random_state=0)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=5, p=2, weights='uniform')\n",
    "    forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=10, max_features='auto', max_depth=10,\n",
    "                                             min_samples_split=2, min_samples_leaf=1, bootstrap=True)\n",
    "    print(\"Cross Validation Score of NB is: %.2f\" % np.mean(cross_val_score(gnb, X, y, cv=skf, n_jobs=1)))\n",
    "    print(\"Cross Validation Score of SVC is: %.2f\" % np.mean(cross_val_score(svc, X, y, cv=skf, n_jobs=1)))\n",
    "    print(\"Cross Validation Score of KNN is: %.2f\" % np.mean(cross_val_score(neigh, X, y, cv=skf, n_jobs=1)))\n",
    "    print(\"Cross Validation Score of Random Forest is: %.2f\" % np.mean(cross_val_score(forest, X, y, cv=skf, n_jobs=1)))\n",
    "\n",
    "    xgboost = xgb.XGBClassifier(max_depth=6, objective='binary:logistic', learning_rate=1, colsample_bytree=1, reg_alpha=5, booster='gbtree')\n",
    "    print(\"Cross Validation Score of XGB is: \", np.mean(cross_val_score(xgboost, X, y, cv=skf, n_jobs=1)))\n",
    "\n",
    "    if self.neural_model:\n",
    "        acc_arr = []\n",
    "        model = keras.Sequential([\n",
    "                keras.layers.Dense(16, input_dim=X.shape[1], activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(16, activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "            _, accuracy = model.evaluate(X_test, y_test)\n",
    "            acc_arr.append(accuracy)\n",
    "        print('Accuracy of this neural network model is: %.2f' % np.mean(acc_arr))\n",
    "        print(acc_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling_custom_kfolds(self, best_features):\n",
    "    gnb = GaussianNB()\n",
    "    svc = SVC(kernel='rbf', gamma=0.1, C=0.1, random_state=0)\n",
    "    neigh = KNeighborsClassifier(n_neighbors=6, p=3, weights='uniform')\n",
    "    forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=5, max_depth=3, bootstrap=True)\n",
    "    xgboost = xgb.XGBClassifier(max_depth=6, objective='binary:logistic', learning_rate=1, colsample_bytree=1, reg_alpha=5, booster='gbtree')\n",
    "    \n",
    "    if self.neural_model:\n",
    "        model = keras.Sequential([\n",
    "                keras.layers.Dense(16, input_dim=len(best_features), activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(16, activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "    result = defaultdict(list)\n",
    "    for i in range(10):\n",
    "        print('Getting {} set: '.format(i + 1))\n",
    "        train_set, test_set = self.get_random_kfolds(train_set_size=0.8)\n",
    "        X_train = train_set[best_features]\n",
    "        y_train = train_set[self.label_type]\n",
    "        y_train = y_train.astype('int')\n",
    "        \n",
    "        X_test = test_set[best_features]\n",
    "        y_test = test_set[self.label_type]\n",
    "        y_test = y_test.astype('int')\n",
    "        \n",
    "        gnb.fit(X_train, y_train)\n",
    "        gnb_pred = gnb.predict(X_test)\n",
    "        result['gnb'].append(mx.accuracy_score(y_test, gnb_pred)) \n",
    "        \n",
    "        svc.fit(X_train, y_train)\n",
    "        svc_pred = svc.predict(X_test)\n",
    "        result['svc'].append(mx.accuracy_score(y_test, svc_pred))\n",
    "        \n",
    "        neigh.fit(X_train, y_train)\n",
    "        neigh_pred = neigh.predict(X_test)\n",
    "        result['neigh'].append(mx.accuracy_score(y_test, neigh_pred))\n",
    "        \n",
    "        forest.fit(X_train, y_train)\n",
    "        forest_pred = forest.predict(X_test)\n",
    "        result['forest'].append(mx.accuracy_score(y_test, forest_pred))\n",
    "        \n",
    "        xgboost.fit(X_train, y_train)\n",
    "        xgboost_pred = xgboost.predict(X_test)\n",
    "        result['xgboost'].append(mx.accuracy_score(y_test, xgboost_pred))\n",
    "        \n",
    "        if self.neural_model:\n",
    "            model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "            model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "            _, accuracy = model.evaluate(X_test, y_test)\n",
    "            result['neuralNetwork'].append(accuracy)\n",
    "    \n",
    "    print(\"Accuracy of NB is: \", np.mean(result['gnb']))\n",
    "    print(\"Accuracy of SVC is: \", np.mean(result['svc']))\n",
    "    print(\"Accuracy of KNN is: \", np.mean(result['neigh']))\n",
    "    print(\"Accuracy of Random Forest is: \", np.mean(result['forest']))\n",
    "    print(\"Accuracy of Xgboost is: \", np.mean(result['xgboost']))\n",
    "    if self.neural_model:\n",
    "        print(\"Accuracy of Neural Network is: \", np.mean(result['neuralNetwork']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_hyperparameters(self, best_features):\n",
    "    res_best = defaultdict(list)\n",
    "    for i in range(10):\n",
    "        print('\\n----------Round {}----------'.format(i + 1))\n",
    "        train_set, test_set = self.get_random_kfolds(train_set_size=0.8)\n",
    "        \n",
    "        X_train = train_set[best_features]\n",
    "        y_train = train_set[self.label_type]\n",
    "        y_train = y_train.astype('int')\n",
    "        \n",
    "        X_test = test_set[best_features]\n",
    "        y_test = test_set[self.label_type]\n",
    "        y_test = y_test.astype('int')\n",
    "        \n",
    "        result = defaultdict(list)\n",
    "        gamma_val = c_val = 0.01\n",
    "        while gamma_val < 10:\n",
    "            while c_val < 10:\n",
    "                svc = SVC(kernel='rbf', gamma=gamma_val, C=c_val, random_state=0)        \n",
    "                svc.fit(X_train, y_train)\n",
    "                svc_pred = svc.predict(X_test)\n",
    "                result[(gamma_val, c_val)].append(np.round(mx.accuracy_score(y_test, svc_pred), 2))\n",
    "                c_val += 0.01\n",
    "            gamma_val += 0.01\n",
    "        final_val = max(result.items(), key= lambda x: x[1])\n",
    "        print(\"Accuracy of SVC is: {}\".format(final_val))\n",
    "        res_best['svc'].append(final_val)\n",
    "        \n",
    "        result = defaultdict(list)\n",
    "        for n_neighbors_val in range(3, 15):\n",
    "            for p_val in range(1, 6):\n",
    "                neigh = KNeighborsClassifier(n_neighbors=n_neighbors_val, p=p_val, weights='uniform')\n",
    "                neigh.fit(X_train, y_train)\n",
    "                neigh_pred = neigh.predict(X_test)\n",
    "                result[(n_neighbors_val, p_val)].append(np.round(mx.accuracy_score(y_test, neigh_pred), 2))\n",
    "        final_val = max(result.items(), key= lambda x: x[1])\n",
    "        print(\"Accuracy of KNN is: {}\".format(final_val))\n",
    "        res_best['knn'].append(final_val)\n",
    "\n",
    "        result = defaultdict(list)\n",
    "        for n_estimators in range(2, 11):\n",
    "            for max_depth in range(2, 11):\n",
    "                forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                                         bootstrap=True)\n",
    "                forest.fit(X_train, y_train)\n",
    "                forest_pred = forest.predict(X_test)\n",
    "                result[(n_estimators, max_depth)].append(np.round(mx.accuracy_score(y_test, forest_pred), 2))\n",
    "        final_val = max(result.items(), key= lambda x: x[1])\n",
    "        print(\"Accuracy of Random forest is: {}\".format(final_val))\n",
    "        res_best['forest'].append(final_val)\n",
    "        \n",
    "        result = defaultdict(list)\n",
    "        for learning_rate in range(1, 100):\n",
    "            for max_depth in range(2, 11):\n",
    "                for reg_alpha in range(1, 10):\n",
    "                    xgboost = xgb.XGBClassifier(max_depth=max_depth, objective='binary:logistic', \n",
    "                                                learning_rate=learning_rate/100, reg_alpha=reg_alpha, booster='gbtree')\n",
    "                    xgboost.fit(X_train, y_train)\n",
    "                    xgboost_pred = xgboost.predict(X_test)\n",
    "                    result[(learning_rate/100, max_depth, reg_alpha)].append(np.round(mx.accuracy_score(y_test, xgboost_pred), 2))\n",
    "        final_val = max(result.items(), key= lambda x: x[1])\n",
    "        print(\"Accuracy of XBoost is: {}\".format(final_val))\n",
    "        res_best['xgboost'].append(final_val)\n",
    "\n",
    "    print('----------Final Values----------')\n",
    "    aggregate_val = defaultdict(list)\n",
    "    for k, v in res_best.items():\n",
    "        aggregate_val[k].extend([0, 0])\n",
    "        total_val = 0\n",
    "        for val in v:\n",
    "            aggregate_val[k][0] += val[0][0] * val[1][0]\n",
    "            aggregate_val[k][1] += val[0][1] * val[1][0]\n",
    "            total_val += val[1][0]\n",
    "        aggregate_val[k][0] /= total_val\n",
    "        aggregate_val[k][1] /= total_val\n",
    "        print('Best Possible Values for {} are: {}'.format(k, aggregate_val[k]))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(self):\n",
    "    self.df = pandas.read_excel(self.fileName, encoding='ansi')\n",
    "    self.__remove_unusable_features__()\n",
    "    self.__get_baseline__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self, label_type, neural_model, fileName):\n",
    "        self.label_type = label_type\n",
    "        self.neural_model = neural_model\n",
    "        self.fileName = fileName\n",
    "        self.df = None\n",
    "    \n",
    "    __remove_unusable_features__ = __remove_unusable_features__\n",
    "    __get_baseline__ = __get_baseline__\n",
    "    __score_model__ = __score_model__\n",
    "    get_random_kfolds = get_random_kfolds\n",
    "    modelling = modelling\n",
    "    modelling_custom_kfolds = modelling_custom_kfolds\n",
    "    get_data = get_data\n",
    "    select_best_features_chi2 = select_best_features_chi2\n",
    "    ablation_test = ablation_test\n",
    "    tuning_hyperparameters = tuning_hyperparameters\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Shape is:  (70, 330)\n",
      "Shape after data cleaning is:  (60, 319)\n",
      "Total rows is=  60\n",
      "Total papers reproducible 25\n",
      "total_true % is=  41.66666666666667\n",
      "total_false % is= 58.333333333333336\n",
      "Getting 1 set: \n",
      "Getting 2 set: \n",
      "Getting 3 set: \n",
      "Getting 4 set: \n",
      "Getting 5 set: \n",
      "Getting 6 set: \n",
      "Getting 7 set: \n",
      "Getting 8 set: \n",
      "Getting 9 set: \n",
      "Getting 10 set: \n",
      "Accuracy of NB is:  0.8333333333333334\n",
      "Accuracy of SVC is:  0.575\n",
      "Accuracy of KNN is:  0.8666666666666666\n",
      "Accuracy of Random Forest is:  0.7166666666666666\n",
      "Accuracy of Xgboost is:  0.6416666666666667\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    all_labels = ['pvalue.label', 'O.within.CI.R', 'Meta.analysis.significant']\n",
    "    mscore = model(all_labels[0], neural_model=False, fileName='data/final_network_data.xlsx')\n",
    "    mscore.get_data()\n",
    "    features = mscore.select_best_features_chi2()\n",
    "    features = list(features['Specs'])[:10]\n",
    "#     mscore.modelling(features)\n",
    "    mscore.modelling_custom_kfolds(features)\n",
    "#     mscore.tuning_hyperparameters(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
