{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import sklearn.metrics as mx\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, label_type, neural_model, fileName):\n",
    "        self.label_type = label_type\n",
    "        self.neural_model = neural_model\n",
    "        self.fileName = fileName\n",
    "        self.df = None\n",
    "\n",
    "    def __score_model__(self, X_train, X_test, y_train, y_test, clf):\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        return mx.f1_score(y_test, y_pred)\n",
    "\n",
    "    def ablation_test(self):\n",
    "        self.__remove_unusable_features__()\n",
    "        X = self.df.drop([self.label_type], axis=1)\n",
    "        y = self.df[self.label_type]\n",
    "\n",
    "        gnb = GaussianNB()\n",
    "        neigh = KNeighborsClassifier(n_neighbors=6, p=2, weights='uniform')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        base_score = dict()\n",
    "        base_score['gnb'] = self.__score_model__(X_train, X_test, y_train, y_test, gnb)\n",
    "        base_score['neigh'] = self.__score_model__(X_train, X_test, y_train, y_test, neigh)\n",
    "\n",
    "        scores = defaultdict(list)\n",
    "        for idx in range(X_train.shape[1]):\n",
    "            cols = [ndx != idx for ndx in range(X_train.shape[1])]\n",
    "            scores['gnb'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, gnb))\n",
    "            scores['neigh'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, neigh))\n",
    "\n",
    "        final_scores_gnb = dict()\n",
    "        final_scores_neigh = dict()\n",
    "        for k, v in scores.items():\n",
    "            if k == 'gnb':\n",
    "                for idx in range(len(v)):\n",
    "                    final_scores_gnb[X.columns[idx]] = (v[idx] - base_score[k])\n",
    "                final_scores_gnb = sorted(final_scores_gnb.items(), key=lambda kv: kv[1], reverse=True)\n",
    "            elif k == 'neigh':\n",
    "                for idx in range(len(v)):\n",
    "                    final_scores_neigh[X.columns[idx]] = (v[idx] - base_score[k])\n",
    "                final_scores_neigh = sorted(final_scores_neigh.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "        print('Based on Naive Bayes: ', final_scores_gnb)\n",
    "        print('Based on KNN: ', final_scores_neigh)\n",
    "\n",
    "    def __remove_unusable_features__(self):\n",
    "        print('Initial Shape is: ', self.df.shape)\n",
    "        if self.label_type == 'pvalue.label':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R', 'Meta.analysis.significant'], axis=1)\n",
    "        elif self.label_type == 'O.within.CI.R':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'Meta.analysis.significant', 'pvalue.label'], axis=1)\n",
    "        elif self.label_type == 'Meta.analysis.significant':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R', 'pvalue.label'], axis=1)\n",
    "\n",
    "        cols_drop = {'DOI', '1st.author.O', 'Senior.author.O', 'Study.Title.O', 'Unnamed: 0', 'Unnamed: 0.1', 'Volume.O'}\n",
    "        cols_total = set(self.df.columns)\n",
    "        self.df = self.df.drop(cols_drop.intersection(cols_total), axis=1)\n",
    "        self.df = self.df.dropna()\n",
    "        print('Shape after data cleaning is: ', self.df.shape)\n",
    "\n",
    "    def __get_baseline__(self):\n",
    "        total_true = total_false = total_val = 0\n",
    "        for idx, row in self.df.iterrows():\n",
    "            total_val += 1\n",
    "            if row[self.label_type] == 1:\n",
    "                total_true += 1\n",
    "            else:\n",
    "                total_false += 1\n",
    "        print(\"Total rows is: \", total_val)\n",
    "        print(\"Total papers reproducible: \", total_true)\n",
    "        print(\"total_true % is: \", (total_true / total_val) * 100)\n",
    "        print(\"total_false % is:\", (total_false / total_val) * 100)\n",
    "\n",
    "    def select_best_features_chi2(self):\n",
    "        X = self.df.drop([self.label_type, 'Authors.O'], axis=1)\n",
    "        cols = X.columns\n",
    "        X = MinMaxScaler().fit_transform(X)\n",
    "        y = self.df[self.label_type]\n",
    "        bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "        fit = bestfeatures.fit(X, y)\n",
    "        dfscores = pandas.DataFrame(fit.scores_)\n",
    "        dfcolumns = pandas.DataFrame(cols)\n",
    "        featureScores = pandas.concat([dfcolumns, dfscores], axis=1)\n",
    "        featureScores.columns = ['Specs', 'Score']\n",
    "        final_features = featureScores.nlargest(featureScores.shape[0] - 2, 'Score')\n",
    "        pandas.set_option('display.max_rows', final_features.shape[0] + 1)\n",
    "        return final_features\n",
    "\n",
    "    def get_random_kfolds(self, train_set_size=0.9, seed=0, authors=None):\n",
    "        if authors is None:\n",
    "            return\n",
    "        df = self.df.copy()\n",
    "        train_set, test_set = pandas.DataFrame(columns=list(df.columns)), pandas.DataFrame(columns=list(df.columns))\n",
    "        while len(train_set) <= self.df.shape[0] * train_set_size and len(authors) > 0 and df.shape[0] > 0:\n",
    "            np.random.seed(seed)\n",
    "            author = authors[np.random.randint(0, len(authors))]\n",
    "            temp_df = df.loc[df['Authors.O'].str.contains(author, case=False)]\n",
    "            train_set = pandas.concat([train_set, temp_df], ignore_index=True)\n",
    "            authors.remove(author)\n",
    "\n",
    "            # remove all the rows from df that have been added to the new set\n",
    "            df.drop(df[df['Authors.O'].str.contains(author, case=False)].index, inplace=True)\n",
    "\n",
    "            if test_set.shape[0] <= self.df.shape[0] * (1 - train_set_size) and len(authors) > 0 and df.shape[0] > 0:\n",
    "                np.random.seed(seed)\n",
    "                author = authors[np.random.randint(0, len(authors))]\n",
    "                temp_df = df.loc[df['Authors.O'].str.contains(author, case=False)]\n",
    "                test_set = pandas.concat([test_set, temp_df], ignore_index=True)\n",
    "                authors.remove(author)\n",
    "\n",
    "                # remove all the rows from df that have been added to the new set\n",
    "                df.drop(df[df['Authors.O'].str.contains(author, case=False)].index, inplace=True)\n",
    "\n",
    "        train_set = train_set.drop(['Authors.O'], axis=1)\n",
    "        test_set = test_set.drop(['Authors.O'], axis=1)\n",
    "        return train_set, test_set\n",
    "\n",
    "    def modelling(self, best_features):\n",
    "        self.df = self.df.drop(['Authors.O'], axis=1)\n",
    "        X = self.df[best_features]\n",
    "        y = self.df[self.label_type]\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "        gnb = GaussianNB()\n",
    "        neigh = KNeighborsClassifier(n_neighbors=6, p=3, weights='uniform')\n",
    "        forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=5, max_features='auto', max_depth=10,\n",
    "                                                 min_samples_split=2, min_samples_leaf=1, bootstrap=True)\n",
    "        print(\"Cross Validation Score of Naive Bayes is: %.2f\" % np.mean(cross_val_score(gnb, X, y, cv=skf, n_jobs=1)))\n",
    "        print(\"Cross Validation Score of KNN is: %.2f\" % np.mean(cross_val_score(neigh, X, y, cv=skf, n_jobs=1)))\n",
    "        print(\"Cross Validation Score of Random Forest is: %.2f\" % np.mean(cross_val_score(forest, X, y, cv=skf, n_jobs=1)))\n",
    "\n",
    "        if self.neural_model:\n",
    "            acc_arr = []\n",
    "            model = keras.Sequential([\n",
    "                keras.layers.Dense(16, input_dim=X.shape[1], activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(16, activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "                _, accuracy = model.evaluate(X_test, y_test)\n",
    "                acc_arr.append(accuracy)\n",
    "            print('Accuracy of this neural network model is: %.2f' % np.mean(acc_arr))\n",
    "            print(acc_arr)\n",
    "\n",
    "    def modelling_custom_kfolds(self, best_features):\n",
    "        gnb = GaussianNB()\n",
    "        neigh = KNeighborsClassifier(n_neighbors=11, p=2, weights='uniform')\n",
    "        forest = ensemble.RandomForestClassifier(random_state=0, bootstrap=True)\n",
    "        model = None\n",
    "        if self.neural_model:\n",
    "            model = keras.Sequential([\n",
    "                keras.layers.Dense(32, input_dim=len(best_features), activation='sigmoid'),\n",
    "                keras.layers.Dense(16, activation='sigmoid'),\n",
    "                keras.layers.Dense(32, activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        result = defaultdict(list)\n",
    "        authors = []\n",
    "        for au in list(self.df['Authors.O']):\n",
    "            for j in au.split(','):\n",
    "                author = j.strip().lower()\n",
    "                if author not in authors:\n",
    "                    authors.append(author)\n",
    "        for idx in range(10):\n",
    "            print('Getting {} set '.format(idx + 1))\n",
    "            train_set, test_set = self.get_random_kfolds(train_set_size=0.8, seed=idx, authors=copy.deepcopy(authors))\n",
    "            X_train = train_set[best_features]\n",
    "            y_train = train_set[self.label_type]\n",
    "            y_train = y_train.astype('int')\n",
    "\n",
    "            X_test = test_set[best_features]\n",
    "            y_test = test_set[self.label_type]\n",
    "            y_test = y_test.astype('int')\n",
    "\n",
    "            gnb.fit(X_train, y_train)\n",
    "            gnb_pred = gnb.predict(X_test)\n",
    "            result['gnb'].append(mx.f1_score(y_test, gnb_pred))\n",
    "\n",
    "            neigh.fit(X_train, y_train)\n",
    "            neigh_pred = neigh.predict(X_test)\n",
    "            result['neigh'].append(mx.f1_score(y_test, neigh_pred))\n",
    "\n",
    "            forest.fit(X_train, y_train)\n",
    "            forest_pred = forest.predict(X_test)\n",
    "            result['forest'].append(mx.f1_score(y_test, forest_pred))\n",
    "\n",
    "            if self.neural_model:\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "                _, accuracy = model.evaluate(X_test, y_test)\n",
    "                result['neuralNetwork'].append(accuracy)\n",
    "\n",
    "        print(\"Accuracy of Naive Bayes is: %0.2f\" % np.mean(result['gnb']))\n",
    "        print(\"Accuracy of KNN is: %0.2f\" % np.mean(result['neigh']))\n",
    "        print(\"Accuracy of Random Forest is: %0.2f\" % np.mean(result['forest']))\n",
    "        if self.neural_model:\n",
    "            print(\"Accuracy of Neural Network is: %0.2f\" % np.mean(result['neuralNetwork']))\n",
    "\n",
    "    def tuning_hyperparameters(self, best_features):\n",
    "        res_best = defaultdict(list)\n",
    "        authors = []\n",
    "        for au in list(self.df['Authors.O']):\n",
    "            for j in au.split(','):\n",
    "                author = j.strip().lower()\n",
    "                if author not in authors:\n",
    "                    authors.append(author)\n",
    "        for idx in range(10):\n",
    "            print('\\n----------Round {}----------'.format(idx + 1))\n",
    "            train_set, test_set = self.get_random_kfolds(train_set_size=0.8, seed=idx, authors=copy.deepcopy(authors))\n",
    "\n",
    "            X_train = train_set[best_features]\n",
    "            y_train = train_set[self.label_type]\n",
    "            y_train = y_train.astype('int')\n",
    "\n",
    "            X_test = test_set[best_features]\n",
    "            y_test = test_set[self.label_type]\n",
    "            y_test = y_test.astype('int')\n",
    "\n",
    "            result = defaultdict(list)\n",
    "            for n_neighbors_val in range(3, 15):\n",
    "                for p_val in range(1, 6):\n",
    "                    neigh = KNeighborsClassifier(n_neighbors=n_neighbors_val, p=p_val, weights='uniform')\n",
    "                    neigh.fit(X_train, y_train)\n",
    "                    neigh_pred = neigh.predict(X_test)\n",
    "                    result[(n_neighbors_val, p_val)].append(np.round(mx.f1_score(y_test, neigh_pred), 2))\n",
    "            final_val = max(result.items(), key=lambda x: x[1])\n",
    "            print(\"Accuracy of KNN is: {}\".format(final_val))\n",
    "            res_best['knn'].append(final_val)\n",
    "\n",
    "            result = defaultdict(list)\n",
    "            for n_estimators in range(2, 11):\n",
    "                for max_depth in range(2, 11):\n",
    "                    forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                                             bootstrap=True)\n",
    "                    forest.fit(X_train, y_train)\n",
    "                    forest_pred = forest.predict(X_test)\n",
    "                    result[(n_estimators, max_depth)].append(np.round(mx.f1_score(y_test, forest_pred), 2))\n",
    "            final_val = max(result.items(), key=lambda x: x[1])\n",
    "            print(\"Accuracy of Random forest is: {}\".format(final_val))\n",
    "            res_best['forest'].append(final_val)\n",
    "\n",
    "        print('----------Final Values----------')\n",
    "        aggregate_val = defaultdict(list)\n",
    "        for k, v in res_best.items():\n",
    "            aggregate_val[k].extend([0, 0])\n",
    "            total_val = 0\n",
    "            for val in v:\n",
    "                aggregate_val[k][0] += val[0][0] * val[1][0]\n",
    "                aggregate_val[k][1] += val[0][1] * val[1][0]\n",
    "                total_val += val[1][0]\n",
    "            aggregate_val[k][0] /= total_val\n",
    "            aggregate_val[k][1] /= total_val\n",
    "            print('Best Possible Values for {} are: {}'.format(k, aggregate_val[k]))\n",
    "\n",
    "    def get_data(self):\n",
    "        self.df = pandas.read_excel(self.fileName, encoding='ansi')\n",
    "        self.__remove_unusable_features__()\n",
    "        self.__get_baseline__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Results for data/final_references_wos_data file----------------\n",
      "Initial Shape is:  (70, 324)\n",
      "Shape after data cleaning is:  (65, 313)\n",
      "Total rows is:  65\n",
      "Total papers reproducible:  47\n",
      "total_true % is:  72.3076923076923\n",
      "total_false % is: 27.692307692307693\n",
      "Getting 1 set \n",
      "Getting 2 set \n",
      "Getting 3 set \n",
      "Getting 4 set \n",
      "Getting 5 set \n",
      "Getting 6 set \n",
      "Getting 7 set \n",
      "Getting 8 set \n",
      "Getting 9 set \n",
      "Getting 10 set \n",
      "Accuracy of Naive Bayes is: 0.88\n",
      "Accuracy of KNN is: 0.84\n",
      "Accuracy of Random Forest is: 0.83\n",
      "----------------Results for data/final_references_mag_data file----------------\n",
      "Initial Shape is:  (70, 324)\n",
      "Shape after data cleaning is:  (69, 313)\n",
      "Total rows is:  69\n",
      "Total papers reproducible:  49\n",
      "total_true % is:  71.01449275362319\n",
      "total_false % is: 28.985507246376812\n",
      "Getting 1 set \n",
      "Getting 2 set \n",
      "Getting 3 set \n",
      "Getting 4 set \n",
      "Getting 5 set \n",
      "Getting 6 set \n",
      "Getting 7 set \n",
      "Getting 8 set \n",
      "Getting 9 set \n",
      "Getting 10 set \n",
      "Accuracy of Naive Bayes is: 0.90\n",
      "Accuracy of KNN is: 0.84\n",
      "Accuracy of Random Forest is: 0.86\n",
      "----------------Results for data/final_citations_mag_data file----------------\n",
      "Initial Shape is:  (70, 324)\n",
      "Shape after data cleaning is:  (70, 313)\n",
      "Total rows is:  70\n",
      "Total papers reproducible:  49\n",
      "total_true % is:  70.0\n",
      "total_false % is: 30.0\n",
      "Getting 1 set \n",
      "Getting 2 set \n",
      "Getting 3 set \n",
      "Getting 4 set \n",
      "Getting 5 set \n",
      "Getting 6 set \n",
      "Getting 7 set \n",
      "Getting 8 set \n",
      "Getting 9 set \n",
      "Getting 10 set \n",
      "Accuracy of Naive Bayes is: 0.90\n",
      "Accuracy of KNN is: 0.83\n",
      "Accuracy of Random Forest is: 0.89\n",
      "----------------Results for data/final_bestfeatures_data file----------------\n",
      "Initial Shape is:  (70, 38)\n",
      "Shape after data cleaning is:  (64, 32)\n",
      "Total rows is:  64\n",
      "Total papers reproducible:  47\n",
      "total_true % is:  73.4375\n",
      "total_false % is: 26.5625\n",
      "Getting 1 set \n",
      "Getting 2 set \n",
      "Getting 3 set \n",
      "Getting 4 set \n",
      "Getting 5 set \n",
      "Getting 6 set \n",
      "Getting 7 set \n",
      "Getting 8 set \n",
      "Getting 9 set \n",
      "Getting 10 set \n",
      "Accuracy of Naive Bayes is: 0.91\n",
      "Accuracy of KNN is: 0.87\n",
      "Accuracy of Random Forest is: 0.84\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    all_labels = ['pvalue.label', 'O.within.CI.R', 'Meta.analysis.significant']\n",
    "#     path = 'data/Synthetic data/5_edges_nonreproducible_to_reproducible/'\n",
    "    path = 'data/'\n",
    "    files = [path + 'final_references_wos_data.xlsx', path + 'final_references_mag_data.xlsx',\n",
    "             path + 'final_citations_mag_data.xlsx', path + 'final_bestfeatures_data.xlsx']\n",
    "#     full_df = pandas.DataFrame()\n",
    "    for i in range(len(files)):\n",
    "        print('----------------Results for {} file----------------'.format(files[i].split('.')[0]))\n",
    "        mscore = Model(all_labels[2], neural_model=False, fileName=files[i])\n",
    "        mscore.get_data()\n",
    "        features = mscore.select_best_features_chi2()\n",
    "        b_features = list(features['Specs'])[:10]\n",
    "        if i < len(files) - 1:\n",
    "            # mscore.modelling(b_features)\n",
    "            mscore.modelling_custom_kfolds(b_features)\n",
    "        else:\n",
    "            # mscore.modelling(list(features['Specs']))\n",
    "            mscore.modelling_custom_kfolds(list(features['Specs']))\n",
    "\n",
    "        # mscore.tuning_hyperparameters(b_features)\n",
    "\n",
    "#         df_temp = pandas.read_excel(files[i], encoding='ansi')\n",
    "#         if i == 0:\n",
    "#             full_df = pandas.concat([full_df, df_temp[['DOI', 'P.value.R', 'Direction.R', 'O.within.CI.R',\n",
    "#                                                        'Meta.analysis.significant', 'pvalue.label',\n",
    "#                                                        'Authors.O']]], axis=1)\n",
    "        \n",
    "#         full_df = pandas.concat([full_df, df_temp[b_features]], axis=1)\n",
    "#     full_df.to_excel('data/final_bestfeatures_data.xlsx')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
