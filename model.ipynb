{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas\n",
    "from sklearn import ensemble\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import sklearn.metrics as mx\n",
    "from matplotlib import pyplot as plt\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, label_type, neural_model, fileName):\n",
    "        self.label_type = label_type\n",
    "        self.neural_model = neural_model\n",
    "        self.fileName = fileName\n",
    "        self.df = None\n",
    "\n",
    "    def __score_model__(self, X_train, X_test, y_train, y_test, clf):\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        return mx.accuracy_score(y_test, y_pred)\n",
    "\n",
    "    def ablation_test(self):\n",
    "        self.__remove_unusable_features__()\n",
    "        X = self.df.drop([self.label_type], axis=1)\n",
    "        y = self.df[self.label_type]\n",
    "\n",
    "        gnb = GaussianNB()\n",
    "        svc = SVC(kernel='rbf', gamma=1, C=0.1, random_state=0)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=6, p=2, weights='uniform')\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        base_score = dict()\n",
    "        base_score['gnb'] = self.__score_model__(X_train, X_test, y_train, y_test, gnb)\n",
    "        base_score['svc'] = self.__score_model__(X_train, X_test, y_train, y_test, svc)\n",
    "        base_score['neigh'] = self.__score_model__(X_train, X_test, y_train, y_test, neigh)\n",
    "\n",
    "        scores = defaultdict(list)\n",
    "        for i in range(X_train.shape[1]):\n",
    "            cols = [ndx != i for ndx in range(X_train.shape[1])]\n",
    "            scores['gnb'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, gnb))\n",
    "            scores['svc'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, svc))\n",
    "            scores['neigh'].append(self.__score_model__(X_train.iloc[:, cols], X_test.iloc[:, cols], y_train, y_test, neigh))\n",
    "\n",
    "        final_scores_gnb = dict()\n",
    "        final_scores_svc = dict()\n",
    "        final_scores_neigh = dict()\n",
    "        for k, v in scores.items():\n",
    "            if k == 'gnb':\n",
    "                for i in range(len(v)):\n",
    "                    final_scores_gnb[X.columns[i]] = (v[i] - base_score[k])\n",
    "                final_scores_gnb = sorted(final_scores_gnb.items(), key=lambda kv: kv[1], reverse=True)\n",
    "            elif k == 'svc':\n",
    "                for i in range(len(v)):\n",
    "                    final_scores_svc[X.columns[i]] = (v[i] - base_score[k])\n",
    "                final_scores_svc = sorted(final_scores_svc.items(), key=lambda kv: kv[1], reverse=True)\n",
    "            elif k == 'neigh':\n",
    "                for i in range(len(v)):\n",
    "                    final_scores_neigh[X.columns[i]] = (v[i] - base_score[k])\n",
    "                final_scores_neigh = sorted(final_scores_neigh.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "        print('Based on NB: ', final_scores_gnb)\n",
    "        print('Based on SVC : ', final_scores_svc)\n",
    "        print('Based on KNN: ', final_scores_neigh)\n",
    "\n",
    "    def __remove_unusable_features__(self):\n",
    "        print('Initial Shape is: ', self.df.shape)\n",
    "        if self.label_type == 'pvalue.label':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R', 'Meta.analysis.significant'], axis=1)\n",
    "        elif self.label_type == 'O.within.CI.R':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'Meta.analysis.significant', 'pvalue.label'], axis=1)\n",
    "        elif self.label_type == 'Meta.analysis.significant':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R', 'pvalue.label'], axis=1)\n",
    "\n",
    "        cols_drop = set(['DOI', '1st.author.O', 'Senior.author.O', 'Study.Title.O', 'Unnamed: 0', 'Unnamed: 0.1', 'Volume.O'])\n",
    "        cols_total = set(self.df.columns)\n",
    "        self.df = self.df.drop(cols_drop.intersection(cols_total), axis=1)\n",
    "        self.df = self.df.dropna()\n",
    "        print('Shape after data cleaning is: ', self.df.shape)\n",
    "\n",
    "    def __get_baseline__(self):\n",
    "        total_true = total_false = total_val = 0\n",
    "        for i, row in self.df.iterrows():\n",
    "            total_val += 1\n",
    "            if row[self.label_type] == 1:\n",
    "                total_true += 1\n",
    "            else:\n",
    "                total_false += 1\n",
    "        print(\"Total rows is= \", total_val)\n",
    "        print(\"Total papers reproducible\", total_true)\n",
    "        print(\"total_true % is= \", (total_true / total_val) * 100)\n",
    "        print(\"total_false % is=\", (total_false / total_val) * 100)\n",
    "\n",
    "    def select_best_features_chi2(self):\n",
    "        X = self.df.drop([self.label_type, 'Authors.O'], axis=1)\n",
    "        cols = X.columns\n",
    "        X = MinMaxScaler().fit_transform(X)\n",
    "        y = self.df[self.label_type]\n",
    "        bestfeatures = SelectKBest(score_func=chi2, k=10)\n",
    "        fit = bestfeatures.fit(X, y)\n",
    "        dfscores = pandas.DataFrame(fit.scores_)\n",
    "        dfcolumns = pandas.DataFrame(cols)\n",
    "        featureScores = pandas.concat([dfcolumns, dfscores], axis=1)\n",
    "        featureScores.columns = ['Specs', 'Score']\n",
    "        final_features = featureScores.nlargest(featureScores.shape[0] - 2, 'Score')\n",
    "        pandas.set_option('display.max_rows', final_features.shape[0] + 1)\n",
    "        return final_features\n",
    "\n",
    "    def get_random_kfolds(self, train_set_size=0.9, seed=0):\n",
    "        df = self.df.copy()\n",
    "        authors = list(set([j.strip() for i in list(df['Authors.O']) for j in i.split(',')]))\n",
    "        train_set, test_set = pandas.DataFrame(columns=list(df.columns)), pandas.DataFrame(columns=list(df.columns))\n",
    "        while len(train_set) <= self.df.shape[0] * train_set_size and len(authors) > 0 and df.shape[0] > 0:\n",
    "            np.random.seed(seed)\n",
    "            author = authors[np.random.randint(0, len(authors))]\n",
    "            temp_df = df.loc[df['Authors.O'].str.contains(author)]\n",
    "            train_set = pandas.concat([train_set, temp_df], ignore_index=True)\n",
    "            authors.remove(author)\n",
    "\n",
    "            # remove all the rows from df that have been added to the new set\n",
    "            df.drop(df[df['Authors.O'].str.contains(author)].index, inplace=True)\n",
    "\n",
    "            if test_set.shape[0] <= self.df.shape[0] * (1 - train_set_size) and len(authors) > 0 and df.shape[0] > 0:\n",
    "                np.random.seed(seed)\n",
    "                author = authors[np.random.randint(0, len(authors))]\n",
    "                temp_df = df.loc[df['Authors.O'].str.contains(author)]\n",
    "                test_set = pandas.concat([test_set, temp_df], ignore_index=True)\n",
    "                authors.remove(author)\n",
    "\n",
    "                # remove all the rows from df that have been added to the new set\n",
    "                df.drop(df[df['Authors.O'].str.contains(author)].index, inplace=True)\n",
    "\n",
    "        train_set = train_set.drop(['Authors.O'], axis=1)\n",
    "        test_set = test_set.drop(['Authors.O'], axis=1)\n",
    "        return train_set, test_set\n",
    "\n",
    "    def modelling(self, best_features):\n",
    "        self.df = self.df.drop(['Authors.O'], axis=1)\n",
    "        X = self.df[best_features]\n",
    "        y = self.df[self.label_type]\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "        gnb = GaussianNB()\n",
    "        svc = SVC(kernel='rbf', gamma=0.1, C=0.1, random_state=0)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=6, p=3, weights='uniform')\n",
    "        forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=5, max_features='auto', max_depth=10,\n",
    "                                                 min_samples_split=2, min_samples_leaf=1, bootstrap=True)\n",
    "        print(\"Cross Validation Score of NB is: %.2f\" % np.mean(cross_val_score(gnb, X, y, cv=skf, n_jobs=1)))\n",
    "        print(\"Cross Validation Score of SVC is: %.2f\" % np.mean(cross_val_score(svc, X, y, cv=skf, n_jobs=1)))\n",
    "        print(\"Cross Validation Score of KNN is: %.2f\" % np.mean(cross_val_score(neigh, X, y, cv=skf, n_jobs=1)))\n",
    "        print(\"Cross Validation Score of Random Forest is: %.2f\" % np.mean(cross_val_score(forest, X, y, cv=skf, n_jobs=1)))\n",
    "\n",
    "        xgboost = xgb.XGBClassifier(max_depth=6, objective='binary:logistic', learning_rate=1, colsample_bytree=1, reg_alpha=5, booster='gbtree')\n",
    "        print(\"Cross Validation Score of XGB is: %.2f\" % np.mean(cross_val_score(xgboost, X, y, cv=skf, n_jobs=1)))\n",
    "\n",
    "        if self.neural_model:\n",
    "            acc_arr = []\n",
    "            model = keras.Sequential([\n",
    "                keras.layers.Dense(16, input_dim=X.shape[1], activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(16, activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "                _, accuracy = model.evaluate(X_test, y_test)\n",
    "                acc_arr.append(accuracy)\n",
    "            print('Accuracy of this neural network model is: %.2f' % np.mean(acc_arr))\n",
    "            print(acc_arr)\n",
    "\n",
    "    def modelling_custom_kfolds(self, best_features):\n",
    "        gnb = GaussianNB()\n",
    "        svc = SVC(kernel='rbf', gamma=0.1, C=0.1, random_state=0)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=6, p=3, weights='uniform')\n",
    "        forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=5, max_depth=10, bootstrap=True)\n",
    "        xgboost = xgb.XGBClassifier(max_depth=6, objective='binary:logistic', learning_rate=1, colsample_bytree=1, reg_alpha=5, booster='gbtree')\n",
    "\n",
    "        if self.neural_model:\n",
    "            model = keras.Sequential([\n",
    "                keras.layers.Dense(32, input_dim=len(best_features), activation='sigmoid'),\n",
    "                keras.layers.Dense(16, activation='sigmoid'),\n",
    "                keras.layers.Dense(32, activation='sigmoid'),\n",
    "                keras.layers.Dense(8, activation='sigmoid'),\n",
    "                keras.layers.Dense(1, activation='sigmoid')\n",
    "            ])\n",
    "        result = defaultdict(list)\n",
    "        for i in range(10):\n",
    "            print('Getting {} set '.format(i + 1))\n",
    "            train_set, test_set = self.get_random_kfolds(train_set_size=0.8, seed=i)\n",
    "            X_train = train_set[best_features]\n",
    "            y_train = train_set[self.label_type]\n",
    "            y_train = y_train.astype('int')\n",
    "\n",
    "            X_test = test_set[best_features]\n",
    "            y_test = test_set[self.label_type]\n",
    "            y_test = y_test.astype('int')\n",
    "\n",
    "            gnb.fit(X_train, y_train)\n",
    "            gnb_pred = gnb.predict(X_test)\n",
    "            result['gnb'].append(mx.accuracy_score(y_test, gnb_pred))\n",
    "\n",
    "            svc.fit(X_train, y_train)\n",
    "            svc_pred = svc.predict(X_test)\n",
    "            result['svc'].append(mx.accuracy_score(y_test, svc_pred))\n",
    "\n",
    "            neigh.fit(X_train, y_train)\n",
    "            neigh_pred = neigh.predict(X_test)\n",
    "            result['neigh'].append(mx.accuracy_score(y_test, neigh_pred))\n",
    "\n",
    "            forest.fit(X_train, y_train)\n",
    "            forest_pred = forest.predict(X_test)\n",
    "            result['forest'].append(mx.accuracy_score(y_test, forest_pred))\n",
    "\n",
    "            xgboost.fit(X_train, y_train)\n",
    "            xgboost_pred = xgboost.predict(X_test)\n",
    "            result['xgboost'].append(mx.accuracy_score(y_test, xgboost_pred))\n",
    "\n",
    "            if self.neural_model:\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "                _, accuracy = model.evaluate(X_test, y_test)\n",
    "                result['neuralNetwork'].append(accuracy)\n",
    "\n",
    "        print(\"Accuracy of NB is: %0.2f\" % np.mean(result['gnb']))\n",
    "        print(\"Accuracy of SVC is: %0.2f\" % np.mean(result['svc']))\n",
    "        print(\"Accuracy of KNN is: %0.2f\" % np.mean(result['neigh']))\n",
    "        print(\"Accuracy of Random Forest is: %0.2f\" % np.mean(result['forest']))\n",
    "        print(\"Accuracy of Xgboost is: %0.2f\" % np.mean(result['xgboost']))\n",
    "        if self.neural_model:\n",
    "            print(\"Accuracy of Neural Network is: %0.2f\" % np.mean(result['neuralNetwork']))\n",
    "\n",
    "    def tuning_hyperparameters(self, best_features):\n",
    "        res_best = defaultdict(list)\n",
    "        for i in range(10):\n",
    "            print('\\n----------Round {}----------'.format(i + 1))\n",
    "            train_set, test_set = self.get_random_kfolds(train_set_size=0.8)\n",
    "\n",
    "            X_train = train_set[best_features]\n",
    "            y_train = train_set[self.label_type]\n",
    "            y_train = y_train.astype('int')\n",
    "\n",
    "            X_test = test_set[best_features]\n",
    "            y_test = test_set[self.label_type]\n",
    "            y_test = y_test.astype('int')\n",
    "\n",
    "            result = defaultdict(list)\n",
    "            gamma_val = c_val = 0.01\n",
    "            while gamma_val < 10:\n",
    "                while c_val < 10:\n",
    "                    svc = SVC(kernel='rbf', gamma=gamma_val, C=c_val, random_state=0)\n",
    "                    svc.fit(X_train, y_train)\n",
    "                    svc_pred = svc.predict(X_test)\n",
    "                    result[(gamma_val, c_val)].append(np.round(mx.accuracy_score(y_test, svc_pred), 2))\n",
    "                    c_val += 0.01\n",
    "                gamma_val += 0.01\n",
    "            final_val = max(result.items(), key=lambda x: x[1])\n",
    "            print(\"Accuracy of SVC is: {}\".format(final_val))\n",
    "            res_best['svc'].append(final_val)\n",
    "\n",
    "            result = defaultdict(list)\n",
    "            for n_neighbors_val in range(3, 15):\n",
    "                for p_val in range(1, 6):\n",
    "                    neigh = KNeighborsClassifier(n_neighbors=n_neighbors_val, p=p_val, weights='uniform')\n",
    "                    neigh.fit(X_train, y_train)\n",
    "                    neigh_pred = neigh.predict(X_test)\n",
    "                    result[(n_neighbors_val, p_val)].append(np.round(mx.accuracy_score(y_test, neigh_pred), 2))\n",
    "            final_val = max(result.items(), key=lambda x: x[1])\n",
    "            print(\"Accuracy of KNN is: {}\".format(final_val))\n",
    "            res_best['knn'].append(final_val)\n",
    "\n",
    "            result = defaultdict(list)\n",
    "            for n_estimators in range(2, 11):\n",
    "                for max_depth in range(2, 11):\n",
    "                    forest = ensemble.RandomForestClassifier(random_state=0, n_estimators=n_estimators, max_depth=max_depth,\n",
    "                                                             bootstrap=True)\n",
    "                    forest.fit(X_train, y_train)\n",
    "                    forest_pred = forest.predict(X_test)\n",
    "                    result[(n_estimators, max_depth)].append(np.round(mx.accuracy_score(y_test, forest_pred), 2))\n",
    "            final_val = max(result.items(), key=lambda x: x[1])\n",
    "            print(\"Accuracy of Random forest is: {}\".format(final_val))\n",
    "            res_best['forest'].append(final_val)\n",
    "\n",
    "            result = defaultdict(list)\n",
    "            for learning_rate in range(1, 100):\n",
    "                for max_depth in range(2, 11):\n",
    "                    for reg_alpha in range(1, 10):\n",
    "                        xgboost = xgb.XGBClassifier(max_depth=max_depth, objective='binary:logistic',\n",
    "                                                    learning_rate=learning_rate / 100, reg_alpha=reg_alpha, booster='gbtree')\n",
    "                        xgboost.fit(X_train, y_train)\n",
    "                        xgboost_pred = xgboost.predict(X_test)\n",
    "                        result[(learning_rate / 100, max_depth, reg_alpha)].append(np.round(mx.accuracy_score(y_test, xgboost_pred), 2))\n",
    "            final_val = max(result.items(), key=lambda x: x[1])\n",
    "            print(\"Accuracy of XBoost is: {}\".format(final_val))\n",
    "            res_best['xgboost'].append(final_val)\n",
    "\n",
    "        print('----------Final Values----------')\n",
    "        aggregate_val = defaultdict(list)\n",
    "        for k, v in res_best.items():\n",
    "            aggregate_val[k].extend([0, 0])\n",
    "            total_val = 0\n",
    "            for val in v:\n",
    "                aggregate_val[k][0] += val[0][0] * val[1][0]\n",
    "                aggregate_val[k][1] += val[0][1] * val[1][0]\n",
    "                total_val += val[1][0]\n",
    "            aggregate_val[k][0] /= total_val\n",
    "            aggregate_val[k][1] /= total_val\n",
    "            print('Best Possible Values for {} are: {}'.format(k, aggregate_val[k]))\n",
    "\n",
    "    def get_data(self):\n",
    "        self.df = pandas.read_excel(self.fileName, encoding='ansi')\n",
    "        self.__remove_unusable_features__()\n",
    "        self.__get_baseline__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    all_labels = ['pvalue.label', 'O.within.CI.R', 'Meta.analysis.significant']\n",
    "    mscore = Model(all_labels[0], neural_model=True, fileName='data/final_network_data.xlsx')\n",
    "    mscore.get_data()\n",
    "    features = mscore.select_best_features_chi2()\n",
    "    features = list(features['Specs'])[:10]\n",
    "#     mscore.modelling(features)\n",
    "    mscore.modelling_custom_kfolds(features)\n",
    "#     mscore.tuning_hyperparameters(features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
