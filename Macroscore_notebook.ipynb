{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Macroscore:\n",
    "    def __init__(self, label_type, feature_type='all', encoding='oneHot', specify_features=False,\n",
    "                 features=None, neural_model=False, fileName=''):\n",
    "        # label_type: {'pvalue.label', 'O.within.CI.R', 'Meta.analysis.significant'}\n",
    "        self.label_type = label_type\n",
    "        # features: {'all', 'common', 'network'}\n",
    "        self.feature_type = feature_type\n",
    "        # encoding: {'oneHot', 'label', 'DictVectroize'}\n",
    "        self.encoding = encoding\n",
    "        # if only specific features are required.\n",
    "        self.specify_features = specify_features\n",
    "        # list of features if specify_features is true\n",
    "        self.features = features\n",
    "        # true if neural network model is required\n",
    "        self.neural_model = neural_model\n",
    "        self.fileName = fileName\n",
    "        self.df = None\n",
    "        self.path_head = '../DataExtraction/WOS/RPPdataConverted'\n",
    "\n",
    "    def __label_addition__(self, row):\n",
    "        try:\n",
    "            if 'X' in str(row['P.value.R']) or str(row['P.value.R']) == 'nan' or 'significant' in str(row['P.value.R']):\n",
    "                return 0\n",
    "            if '<' in str(row['P.value.R']):\n",
    "                val = \"{0:.4f}\".format(float(row['P.value.R'].split('<')[1].strip()) - float(row['P.value.R'].split('<')[1].strip()) / 10)\n",
    "            elif '>' in str(row['P.value.R']):\n",
    "                val = \"{0:.4f}\".format(float(row['P.value.R'].split('>')[1].strip()) + float(row['P.value.R'].split('>')[1].strip()) / 10)\n",
    "            elif '=' in str(row['P.value.R']):\n",
    "                val = row['P.value.R'].split('=')[1].strip()\n",
    "            elif 'not significant' in str(row['P.value.R']) and row['Direction.R'] == 'same':\n",
    "                return 1\n",
    "            elif float(row['P.value.R']) <= 0.05 and row['Direction.R'] == 'same':\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "            if float(val) <= 0.05 and row['Direction.R'] == 'same':\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        except ValueError:\n",
    "            return 0\n",
    "\n",
    "    def __clean_pvalue__(self, prop):\n",
    "        for i, row in self.df.iterrows():\n",
    "            if 'significant' in str(row[prop]):\n",
    "                self.df.at[i, prop] = '1'\n",
    "            if '<' in str(row[prop]):\n",
    "                self.df.at[i, prop] = \"{0:.4f}\".format(float(row[prop].split('<')[1].strip()) - float(row[prop].split('<')[1].strip()) / 2)\n",
    "            elif '>' in str(row[prop]):\n",
    "                self.df.at[i, prop] = \"{0:.4f}\".format(float(row[prop].split('>')[1].strip()) + float(row[prop].split('>')[1].strip()) / 2)\n",
    "            elif '=' in str(row[prop]):\n",
    "                self.df.at[i, prop] = row[prop].split('=')[1].strip()\n",
    "            elif 'not significant' in str(row[prop]):\n",
    "                self.df.at[i, prop] = '0'\n",
    "\n",
    "    def __clean_float_result__(self, prop):\n",
    "        for i, row in self.df.iterrows():\n",
    "            try:\n",
    "                if float(row[prop]):\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                self.df.at[i, prop] = 0\n",
    "\n",
    "    def __clean_effect_size__(self, prop):\n",
    "        for i, row in self.df.iterrows():\n",
    "            if '=' in str(row[prop]):\n",
    "                self.df.at[i, prop] = float(str(row[prop]).split('=')[1].strip())\n",
    "\n",
    "    def __get_baseline__(self):\n",
    "        total_true = total_false = total_val = 0\n",
    "        for i, row in self.df.iterrows():\n",
    "            total_val += 1\n",
    "            if row[self.label_type] == 1:\n",
    "                total_true += 1\n",
    "            else:\n",
    "                total_false += 1\n",
    "        print(\"Total rows is= \", total_val)\n",
    "        print(\"total_true % is= \", (total_true / total_val) * 100)\n",
    "        print(\"total_false % is=\", (total_false / total_val) * 100)\n",
    "\n",
    "    def __clean_data__(self):\n",
    "        self.df = self.df.replace(to_replace='X', value=np.nan)\n",
    "        self.df = self.df.dropna(subset=['DOI', 'Reported.P.value.O', 'Direction.R', 'Meta.analysis.significant', 'O.within.CI.R'])\n",
    "        self.df = self.df.replace(to_replace=np.nan, value=0)\n",
    "        self.__clean_pvalue__('Reported.P.value.O')\n",
    "        self.__clean_float_result__('Surprising.result.O')\n",
    "        self.__clean_float_result__('Exciting.result.O')\n",
    "        self.__clean_effect_size__('Effect.size.O')\n",
    "\n",
    "        if self.label_type == 'pvalue.label':\n",
    "            self.df['pvalue.label'] = self.df.apply(lambda paper: self.__label_addition__(paper), axis=1)\n",
    "\n",
    "    def __remove_unusable_features__(self):\n",
    "        if self.label_type == 'pvalue.label':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R', 'Meta.analysis.significant'], axis=1)\n",
    "        elif self.label_type == 'O.within.CI.R':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'Meta.analysis.significant'], axis=1)\n",
    "        elif self.label_type == 'Meta.analysis.significant':\n",
    "            self.df = self.df.drop(['P.value.R', 'Direction.R', 'O.within.CI.R'], axis=1)\n",
    "\n",
    "        cols_drop = set(['DOI', '1st.author.O', 'Senior.author.O', 'Authors.O', 'Study.Title.O', 'Unnamed: 0', 'new_feature_301'])\n",
    "        cols_total = set(self.df.columns)\n",
    "        self.df = self.df.drop(cols_drop.intersection(cols_total), axis=1)\n",
    "        # self.df = self.df.replace(to_replace=np.nan, value=0)\n",
    "        self.df = self.df.dropna()\n",
    "        print('Shape is: ', self.df.shape)\n",
    "\n",
    "    def __get__common_features__(self):\n",
    "        self.__clean_data__()\n",
    "\n",
    "        self.__get_baseline__()\n",
    "\n",
    "        # Changing the data-types of some fields so as not to include them in encoding\n",
    "        self.df = self.df.astype({'Reported.P.value.O': 'float64', 'Exciting.result.O': 'float64', 'Surprising.result.O': 'float64', 'N.O': 'float64',\n",
    "                                  'Effect.size.O': 'float64'})\n",
    "        return self.df\n",
    "\n",
    "    def __get_network_features__(self):\n",
    "        print('Initial Shape is: ', self.df.shape)\n",
    "        if self.feature_type == 'network':\n",
    "            self.__clean_data__()\n",
    "            self.__get_baseline__()\n",
    "        for i, row in self.df.iterrows():\n",
    "            name = row['DOI'].replace('/', '_')\n",
    "            print('----- Getting details for ', row['DOI'], '-------')\n",
    "\n",
    "            # Get Citations related data\n",
    "            file_name = self.path_head + '/{}/paper_{}.txt'.format(name, name)\n",
    "            all_authors_paper = []\n",
    "            if os.path.exists(file_name):\n",
    "                df_doi = pandas.read_csv(file_name, sep='\\t', lineterminator='\\r', encoding=\"utf-16le\", index_col=False,\n",
    "                                         quotechar=None, quoting=3, usecols=['AU', 'NR', 'TC'])\n",
    "                df_doi = df_doi.dropna()\n",
    "                all_authors_paper = df_doi['AU'].str.split(';')[0]\n",
    "                self.df.at[i, 'References'] = df_doi['NR'][0]\n",
    "                self.df.at[i, 'Citation.count.paper.O'] = df_doi['TC'][0]\n",
    "                print('Total references', df_doi['NR'][0])\n",
    "                print('Total Citations', df_doi['TC'][0])\n",
    "\n",
    "            # Get References data: References to same authors\n",
    "            file_name = self.path_head + '/{}/citations_{}.txt'.format(name, name)\n",
    "            if os.path.exists(file_name):\n",
    "                df_doi = pandas.read_csv(file_name, sep='\\t', lineterminator='\\r', encoding=\"utf-16le\", index_col=False,\n",
    "                                         quotechar=None, quoting=3, usecols=['AU'])\n",
    "                df_doi = df_doi.dropna()\n",
    "                cnt_authors = 0\n",
    "                for _, row_internal in df_doi.iterrows():\n",
    "                    authors_ref = row_internal['AU'].split(';')\n",
    "                    for j in authors_ref:\n",
    "                        if j in all_authors_paper:\n",
    "                            cnt_authors += 1\n",
    "                self.df.at[i, 'References.to.self'] = cnt_authors\n",
    "                print('Where they have referred themselves: ', cnt_authors)\n",
    "\n",
    "            # Get Authors data\n",
    "            for author_col in ['1st.author.O', 'Senior.author.O']:\n",
    "                folder_name = self.path_head + '/{}/'.format(name)\n",
    "                keyword = row[author_col]\n",
    "                if os.path.exists(folder_name):\n",
    "                    for file in os.listdir(folder_name):\n",
    "                        if keyword in file:\n",
    "                            df_doi = pandas.read_csv(folder_name + '/' + file, sep='\\t', lineterminator='\\r', encoding=\"utf-16le\", index_col=False,\n",
    "                                                     quotechar=None, quoting=3, usecols=['TC'])\n",
    "                            df_doi = df_doi.dropna()\n",
    "                            self.df.at[i, author_col + ' papers'] = cnt_authors\n",
    "                            print('Number of papers of ' + author_col + ' :', df_doi.shape[0])\n",
    "                            res = 0\n",
    "                            for _, row_internal in df_doi.iterrows():\n",
    "                                res += row_internal['TC']\n",
    "                            self.df.at[i, author_col + ' citations.of.all.papers'] = res\n",
    "                            print('Number of Citations of  ' + author_col + ' :', res)\n",
    "                            break\n",
    "\n",
    "    def get_data(self):\n",
    "        if not self.specify_features:\n",
    "            self.df = pandas.read_excel(self.fileName, encoding='ansi')\n",
    "        else:\n",
    "            self.df = pandas.read_excel(self.fileName, encoding='ansi', usecols=self.features)\n",
    "\n",
    "    def get_feature(self):\n",
    "        if self.feature_type.lower() == 'common':\n",
    "            self.__get__common_features__()\n",
    "        elif self.feature_type.lower() == 'network':\n",
    "            self.__get_network_features__()\n",
    "        elif self.feature_type.lower() == 'all':\n",
    "            self.__get__common_features__()\n",
    "            self.__get_network_features__()\n",
    "        else:\n",
    "            print('Wrong features asked: ', self.features)\n",
    "            return\n",
    "        print('Final Shape is: ', self.df.shape)\n",
    "        self.df.to_excel('data/new_data.xlsx')\n",
    "\n",
    "    def modelling(self):\n",
    "        self.__remove_unusable_features__()\n",
    "        self.__get_baseline__()\n",
    "        X = self.df.drop([self.label_type], axis=1)\n",
    "        y = self.df[self.label_type]\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
    "\n",
    "        gnb = GaussianNB()\n",
    "        Svc = SVC(kernel='rbf', gamma=1, C=0.1, random_state=0)\n",
    "        neigh = KNeighborsClassifier(n_neighbors=6, p=2, weights='uniform')\n",
    "        print(\"Cross Validation Score of NB is: \", np.mean(cross_val_score(gnb, X, y, cv=skf, n_jobs=1)))\n",
    "        print(\"Cross Validation Score of SVC is: \", np.mean(cross_val_score(Svc, X, y, cv=skf, n_jobs=1)))\n",
    "        print(\"Cross Validation Score of KNN is: \", np.mean(cross_val_score(neigh, X, y, cv=skf, n_jobs=1)))\n",
    "\n",
    "        if self.neural_model:\n",
    "            acc_arr = []\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "                model = keras.Sequential([\n",
    "                    keras.layers.Dense(1024, input_dim=X.shape[1], activation='sigmoid'),\n",
    "                    keras.layers.Dense(512, activation='sigmoid'),\n",
    "                    keras.layers.Dense(256, activation='sigmoid'),\n",
    "                    keras.layers.Dense(512, activation='sigmoid'),\n",
    "                    keras.layers.Dense(128, activation='sigmoid'),\n",
    "                    keras.layers.Dense(1, activation='sigmoid')\n",
    "                ])\n",
    "                model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "                model.fit(X_train, y_train, epochs=150, batch_size=10, verbose=0)\n",
    "                _, accuracy = model.evaluate(X_test, y_test)\n",
    "                acc_arr.append(accuracy)\n",
    "            print('Accuracy of this neural network model is: %.2f' % (np.mean(acc_arr) * 100))\n",
    "            print(acc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    req_columns = ['Study.Title.O', 'Volume.O', 'Citation.count.paper.O', 'Number.of.Authors.O', 'DOI', 'Citation.Count.1st.author.O',\n",
    "                   'Reported.P.value.O', 'Exciting.result.O', 'Surprising.result.O', 'N.O', 'Effect.size.O',\n",
    "                   'Institution.prestige.1st.author.O', 'Institution.prestige.senior.author.O', 'O.within.CI.R', 'P.value.R',\n",
    "                   'Direction.R', 'Meta.analysis.significant', 'Citation.count.senior.author.O', '1st.author.O',\n",
    "                   'Senior.author.O', 'Authors.O']\n",
    "    # Uncomment below to clean data and add network features: Step-1\n",
    "    # mscore = Macroscore('pvalue.label', feature_type='all', specify_features=True, features=req_columns,\n",
    "    #                     neural_model=True, fileName='data/RPPdata.xlsx')\n",
    "    # mscore.get_data()\n",
    "    # mscore.get_feature()\n",
    "    \n",
    "    # Uncomment below to train and test our models: Step-5\n",
    "    mscore = Macroscore('pvalue.label', feature_type='all', specify_features=False,\n",
    "                        neural_model=True, fileName='data/final_network_data.xlsx')\n",
    "    mscore.get_data()\n",
    "    mscore.modelling()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
